{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing GPT2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dhia.Garbaya\\AppData\\Local\\miniconda3\\envs\\torch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dhia.Garbaya\\AppData\\Local\\miniconda3\\envs\\torch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dhia.Garbaya\\.cache\\huggingface\\hub\\models--openai-community--gpt2-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# load gpt2 fast-tokenizer and gpt2 large (770M) model (model took > 30min ot load in local, 10s in kaggle)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")\n",
    "\n",
    "# Get embedding matrix and save it locally to speeed up future uses\n",
    "word_embeddings = model.transformer.wte.weight      # 50K: vocab size  x  1280: d_model\n",
    "torch.save(word_embeddings, 'word_embeddings.pt')   # fp32  ;  ~256MB = 4 x (50Kx1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_small = tokenizer\n",
    "model_small = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the embedding matrix: torch.Size([50257, 1280])\n",
      "\n",
      "Chunk of the embedding of the word 'hello':\n",
      " tensor([-0.2132,  0.0170, -0.0069,  0.1358, -0.0563,  0.0098, -0.0777,  0.0074,\n",
      "        -0.0020,  0.0942], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Load embedding matrix on gpu\n",
    "word_embeddings = torch.load('word_embeddings.pt', map_location=torch.device('cuda'), weights_only=True)\n",
    "print(f\"shape of the embedding matrix: {word_embeddings.shape}\\n\")\n",
    "print(f\"Chunk of the embedding of the word 'hello':\\n {word_embeddings[tokenizer('hello')['input_ids'][0]][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_vocab(token, tokenizer):\n",
    "    existence = tokenizer.get_vocab().get(token) is not None\n",
    "    print(f\"'{token}' exists in the vocabulary !\\n\") if existence else print(f\"'{token}' does NOT exist in the vocabulary...\\n\")\n",
    "    return existence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: the token `vector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'vector' exists in the vocabulary !\n",
      "\n",
      "Top 20 most similar words to 'vector': \n",
      "vector; Ġvector; Vector; Ġvectors; ĠVector; vec; string; array; Ġvec; template; sequence; sector; pointer; dimensional; iterator; Orderable; isSpecial; factor; map; aditional\n",
      "\n",
      "Top 20 most similar words to 'vector' using euclidian distance: \n",
      "vector; Ġvector; Vector; Ġvectors; ĠVector; the; what; What; string; It; which; from; for; this; that; Although; For; Ġvec; We; This\n"
     ]
    }
   ],
   "source": [
    "# Encoding for token 'vector'\n",
    "if is_in_vocab(\"vector\", tokenizer):\n",
    "    vector_token = tokenizer('vector')['input_ids'][0]\n",
    "    vector_embedding = word_embeddings[vector_token]\n",
    "\n",
    "    # Most cosine-similar tokens to 'vector' in the vocabulary\n",
    "    similarity = torch.cosine_similarity(word_embeddings, vector_embedding.unsqueeze(0), dim=1)\n",
    "    top_similarities, top_indices = torch.topk(similarity, 20)\n",
    "    top_words = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "    top_words = '; '.join(top_words)\n",
    "    print(f\"Top 20 most similar words to 'vector': \\n{top_words}\")\n",
    "\n",
    "    # Most EucDistance-similar tokens to 'vector' in the vocabulary\n",
    "    similarity = torch.cdist(word_embeddings, vector_embedding.unsqueeze(0))\n",
    "    top_similarities, top_indices = torch.topk(-similarity[:,0], 20) # similarity = - distance\n",
    "    top_words = tokenizer.convert_ids_to_tokens(top_indices)\n",
    "    top_words = '; '.join(top_words)\n",
    "    print(f\"\\nTop 20 most similar words to 'vector' using euclidian distance: \\n{top_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'square' exists in the vocabulary !\n",
      "\n",
      "'triangle' does NOT exist in the vocabulary...\n",
      "\n",
      "'ball' exists in the vocabulary !\n",
      "\n",
      "'cercle' does NOT exist in the vocabulary...\n",
      "\n",
      "'function' exists in the vocabulary !\n",
      "\n",
      "'converge' does NOT exist in the vocabulary...\n",
      "\n",
      "'diverge' does NOT exist in the vocabulary...\n",
      "\n",
      "'positive' exists in the vocabulary !\n",
      "\n",
      "'matrix' does NOT exist in the vocabulary...\n",
      "\n",
      "'sequence' exists in the vocabulary !\n",
      "\n",
      "'integer' exists in the vocabulary !\n",
      "\n",
      "\n",
      "'carré' does NOT exist in the vocabulary...\n",
      "\n",
      "'triangle' does NOT exist in the vocabulary...\n",
      "\n",
      "'boule' does NOT exist in the vocabulary...\n",
      "\n",
      "'cercle' does NOT exist in the vocabulary...\n",
      "\n",
      "'fonction' does NOT exist in the vocabulary...\n",
      "\n",
      "'converge' does NOT exist in the vocabulary...\n",
      "\n",
      "'diverge' does NOT exist in the vocabulary...\n",
      "\n",
      "'positif' does NOT exist in the vocabulary...\n",
      "\n",
      "'matrice' does NOT exist in the vocabulary...\n",
      "\n",
      "'suite' does NOT exist in the vocabulary...\n",
      "\n",
      "'entier' does NOT exist in the vocabulary...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_math_list = ['square', 'triangle', 'ball', 'cercle', 'function', 'converge', 'diverge', 'positive', 'matrix', 'sequence', 'integer', ]\n",
    "fre_math_list = ['carré', 'triangle', 'boule', 'cercle', 'fonction', 'converge', 'diverge', 'positif', 'matrice', 'suite', 'entier', ]\n",
    "for word in eng_math_list:\n",
    "    is_in_vocab(word, tokenizer)\n",
    "print()\n",
    "for word in fre_math_list:\n",
    "    is_in_vocab(word, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GPT2 wasn't a multilangual model. Its vocabulary meaningful tokens mainly consist of english words.\n",
    "* Given the tokenization algorithm (BPE), many words can be split or packed and this won't exist in vocabulary\n",
    "\n",
    "* **Let's choose a small set of english vocabulary among the vocabulary suggested in the paper data, make sure they're all represented by single token and analyze their pairwise similarity**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
